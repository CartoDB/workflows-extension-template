# Running tests

This document goes over the basic steps to create tests for the components included in your extension

## Data Warehouse configuration

For running the `test` and `capture` scripts, you need to configure the access to the data warehouse where your extension is supposed to run. To do so, rename the `.env.template` file in the root of the repository to `.env` and edit it with the appropriate values for each provider (BigQuery or Snowflake).

For BigQuery, we only need to specify the project and dataset where the tests will run.

```
BQ_TEST_PROJECT=
BQ_TEST_DATASET=
```

Check [this section](./tooling.md#authentication-with-the-data-warehouse) to ensure you have authenticated correctly with BigQuery.

For Snowflake, we also need to set credentials to authenticate in the `.env` file.

```
SF_ACCOUNT=
SF_TEST_DATABASE=
SF_TEST_SCHEMA=
SF_USER=
SF_PASSWORD=
```

## Files and folder structure

The content of the [`/components/<component_name>/test/`](../components/template/test/) is as follows:

```
test/
    ├── test.json
    ├── table1.ndjson
    └── fixtures/
        ├── 1.json
        └── 2.json
```

### `test.json`

Contains an array with the definition of each test, specifying the `id` of each test and the values for each input:

```json
[
    {
        "id": 1,
        "inputs": {
            "input_table": "table1",
            "value": "test"
        }
    },
    {
        "id": 2,
        "inputs": {
            "input_table": "table1",
            "value": "test2"
        }
    }
]

You can also add an `env_vars` property, in case you need to pass test environment variables. This property is not mandatory. If missing, and empty dictionary will be passed.

[
    {
        "id": 1,
        "inputs": {
            "input_table": "table1",
            "value": "test"
        },
        "env_vars": {
          "analyticsToolboxDataset": "myproject.mydataset"
        }
    }
]
```

#### Setup Tables

You can also add a `setup_tables` property to define additional tables that should be created before running the test, but are not directly used as component inputs. This is useful when your component needs auxiliary tables or when testing with shared data across multiple tests.

```json
[
    {
        "id": 1,
        "inputs": {
            "input_table": "table1",
            "value": "test"
        },
        "setup_tables": {
            "reference_data": "reference",
            "lookup_table": "lookup"
        }
    }
]
```

In this example:
- `reference_data` and `lookup_table` are the table names that will be created in the test database
- `reference` and `lookup` are the corresponding NDJSON filenames (will look for `reference.ndjson` and `lookup.ndjson` in the test folder)
- These tables will be available as `project.dataset.reference_data` and `project.dataset.lookup_table` in your SQL code (with clean names, no prefixes)
- When referencing setup tables as input parameters, use the table name key (e.g., `"input_table": "reference_data"`)

```

### `table1.ndjson`

An NDJSON file that contains the data to be used in the test. It can have any arbitrary name, but make sure it's correctly referenced in `input_table` in your `test.json` file. For example:

```json
{"id":1,"name":"Alice"}
{"id":2,"name":"Bob"}
{"id":3,"name":"Carol"}
```

### `fixtures/<id>.json`

The fixture files contain the expected result for each test defined in `test.json`. For example, for our test `1` we would have a `1.json` file with this content:

```json
{
    "output_table": [
        {
            "name": "Bob",
            "id": 2,
            "fixed_value_col": "test"
        },
        {
            "name": "Carol",
            "id": 3,
            "fixed_value_col": "test"
        },
        {
            "name": "Alice",
            "id": 1,
            "fixed_value_col": "test"
        }
    ]
}
```

When developing new components, the `fixture` folder and its content will be automatically generated by running the `capture` command:

```bash
$ python carto_extension.py capture
```

## Setup

Setup the elements in the `test` folder to define how the test should be run to verify that the component is correctly working.
Checkout [this section](./anatomy_of_an_extension.md#test) to understand which files are necessary to define the tests.

Run the `capture` script to create the test fixtures from the results of running your components in the corresponding datawarehouse.

```bash
$ python carto_extension.py capture
```

This command will generate fixture files in the `fixtures` folder.
Check the created files to ensure that the output is as expected.

From that point, you can now run the `test` script to run tests and check if they match the captured outputs, whenever you change the implementation of any of the components.

```bash
$ python carto_extension.py test
```

## CI Configuration

This template includes a GitHub workflow to run the extension test suite when new changes are pushed to the repository (provided that the `capture` script has been run and test fixtures have been captured).

GitHub secrets must be configured in order to have the workflow correctly running. Check the [`.github/workflows/test.yml`](../.github/workflows/test.yml) file for more information.

### Required Secrets

For BigQuery testing:
- `BQ_TEST_PROJECT`: BigQuery project for testing
- `BQ_TEST_DATASET`: BigQuery dataset for testing
- `GOOGLE_CREDENTIALS`: Service account credentials JSON

For Snowflake testing:
- `SF_ACCOUNT`: Snowflake account identifier
- `SF_TEST_DATABASE`: Snowflake database for testing
- `SF_TEST_SCHEMA`: Snowflake schema for testing
- `SF_USER`: Snowflake username
- `SF_PASSWORD`: Snowflake password
