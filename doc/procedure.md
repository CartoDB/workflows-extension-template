---
title: Writing Component Stored Procedures
description: Guide to implementing component logic in fullrun.sql and dryrun.sql with SQL patterns and best practices
version: 1.0.0
last-updated: 2025-01-27
depends-on: [component_metadata.md]
tags: [sql, stored-procedures, logic, implementation, patterns]
see-also: [glossary.md]
---

# Writing the component procedure

The component's logic should be implemented in the `fullrun.sql` and `dryrun.sql` files.

> **üí° Example:** See the [annotated template files](../components/template/src/) for detailed pattern explanations with comments.
>
> **ü§ñ For AI Agents:** Check [Quick Reference](./reference/quick-reference.md) for SQL pattern snippets and [Validation Rules](./reference/validation-rules.md) for SQL constraints.

## Dry runs

Workflows needs to perform a dry-run query before the actual execution of the workflow, in order to determine the resulting schema of each node.

For this, we need to create a `dryrun.sql` file that generates an empty table with the same schema as the actual component's result (defined in `fullrun.sql`) and returning 0 rows.

> üö® **CRITICAL REQUIREMENT: Schema Must Match Exactly**
>
> **The dryrun.sql MUST produce EXACTLY the same output schema as fullrun.sql:**
> - ‚úÖ Same column names (case-sensitive)
> - ‚úÖ Same column types
> - ‚úÖ Same column order
> - ‚úÖ Same number of columns
> - ‚úÖ Zero rows (use `WHERE 1 = 0`)
>
> **The query implementation can differ** from fullrun.sql to optimize performance:
> - Replace expensive functions with cheap literals of the same type (e.g., `"uuid_string"` instead of `GENERATE_UUID()`)
> - Simplify computations while maintaining output types
> - BUT: Always reference the same input tables and maintain the same SELECT structure
>
> **Golden Rule:** Generate exactly the same schema as the full run, as simply as possible.

Below you can see an example of a stored procedure built following the approach defined above. This procedure takes a table and generates a new one that includes and additional column with a unique identifier.

### `fullrun.sql`

```sql
        EXECUTE IMMEDIATE '''
        CREATE TABLE IF NOT EXISTS ''' || output || '''
        OPTIONS (expiration_timestamp = TIMESTAMP_ADD(CURRENT_TIMESTAMP(), INTERVAL 30 DAY))
        AS SELECT *, GENERATE_UUID() AS uuid
        FROM ''' || input || ';';
```

### `dryrun.sql`

```sql
        EXECUTE IMMEDIATE '''
        CREATE TABLE IF NOT EXISTS ''' || output || '''
        OPTIONS (
                expiration_timestamp = TIMESTAMP_ADD(CURRENT_TIMESTAMP(), INTERVAL 30 DAY))
        AS SELECT *, "uuid_string" AS uuid
        FROM ''' || input || '''
        WHERE 1 = 0;
        ''';
```

### Common Dry Run Mistakes to Avoid

‚ùå **WRONG: Query without FROM clause**
```sql
-- This generates a completely different schema!
CREATE TABLE output AS
SELECT "uuid_string" AS uuid
WHERE 1 = 0;
```

‚ùå **WRONG: Different column names**
```sql
-- fullrun.sql creates 'uuid', dryrun.sql creates 'id' - SCHEMA MISMATCH!
SELECT *, "uuid_string" AS id
FROM input_table
WHERE 1 = 0;
```

‚ùå **WRONG: Different column order**
```sql
-- fullrun.sql: SELECT col_a, col_b, new_col
-- dryrun.sql: SELECT new_col, col_a, col_b  -- WRONG ORDER!
```

‚ùå **WRONG: Missing WHERE 1 = 0**
```sql
-- This will process all data, defeating the purpose of dry run!
SELECT *, "uuid_string" AS uuid
FROM input_table;
```

‚úÖ **CORRECT: Same schema, optimized execution**
```sql
-- Matches fullrun.sql schema exactly:
-- - Same SELECT * to preserve input columns
-- - Same "AS uuid" column name
-- - Same STRING type for uuid column
-- - Adds WHERE 1 = 0 for zero rows
SELECT *, "uuid_string" AS uuid
FROM input_table
WHERE 1 = 0;
```

## Variables

For implementing the logic of the component, you will have available a set of variable matching the names of the parameters declared in the `metadata.json` file, with both input and output parameters. These variables will contain the values selected by the user when configuring the component as part of a workflow.

If you have declared environment variables in your component metadata (see [here](./component_metadata.md#cartoenvvars)), they will also be created and they will be available to be used from your code.

The `input` and `output` variables will contain the names of the input table that was configured for the component, and the name of the output one generated by the component. Parameters with those same names should have been declared in the corresponding `metadata.json`file.

Do not generate tables with names others than the ones provided in the variables corresponding to output parameters. Otherwise, those tables will not be used when the component output is connected to another component in the workflow.

## Referencing CARTO Analytics Toolbox

If your component uses CARTO Analytics Toolbox functions, you need to choose the appropriate approach based on how your component is structured:

### Approach 1: Using the @@analytics_toolbox_location@@ placeholder

Use the `@@analytics_toolbox_location@@` placeholder directly in your stored procedure SQL:

```sql
CREATE OR REPLACE PROCEDURE component_procedure()
BEGIN
  SELECT @@analytics_toolbox_location@@.FUNCTION_NAME(geography_column)
  FROM input_table;
END;
```

**How it works:**
- The placeholder `@@analytics_toolbox_location@@` is **replaced at extension installation time** in the Workflows UI
- The FQN is **baked into the stored procedure definition** when the extension is installed
- This substitution happens **once** when the user installs your extension
- The placeholder is replaced with the connection-specific location (e.g., `carto-un.carto`)

**When to use:**
- When the Analytics Toolbox FQN needs to be part of the procedure definition itself
- **Required for BigQuery** when function references are in the procedure body
- **BigQuery only** - not available for other providers

### Approach 2: Using cartoEnvVars

Declare `analyticsToolboxDataset` in your component's `cartoEnvVars` array:

```json
{
  "cartoEnvVars": ["analyticsToolboxDataset"],
  ...
}
```

Then use it as a variable in dynamic SQL:

```sql
EXECUTE IMMEDIATE '''
SELECT ''' || analyticsToolboxDataset || '''.FUNCTION_NAME(geography_column)
FROM ''' || input_table;
```

**How it works:**
- The variable `analyticsToolboxDataset` is **evaluated at workflow execution time**
- Workflows injects the correct value when building the SQL dynamically
- The value can adapt if connection settings change

**When to use:**
- When building SQL dynamically with `EXECUTE IMMEDIATE`
- Works across BigQuery, Snowflake, and Oracle
- When you need flexibility for the location to change after installation

### Key Differences

| Aspect | Placeholder | cartoEnvVars |
|--------|-------------|--------------|
| **When evaluated** | At extension installation (static) | At workflow execution (dynamic) |
| **Use case** | FQN in procedure definition | FQN in dynamic SQL |
| **Flexibility** | Fixed at installation | Can change with connection settings |
| **Availability** | **BigQuery only** | BigQuery, Snowflake, Oracle |
| **Context** | Baked into stored procedure | Evaluated when building SQL |

### For Local Testing

When running `python carto_extension.py test` or `deploy` locally, you can define the placeholder in a `.env` file:

```
analytics_toolbox_location=carto-un.carto
```

This is purely for your development workflow and has no effect on how the extension works for end users who install it from CARTO UI.


## Table Names and Execution Contexts

Your component will run in **two different execution contexts**, and table names are formatted differently in each:

### UI Execution Mode (Interactive)

**When:** User runs a workflow from the Workflows UI

**Table name format:** Fully Qualified Names (FQNs)
- BigQuery: `project.dataset.table`
- Snowflake: `DATABASE.SCHEMA.TABLE`

**Example:**
```sql
-- input_table might contain:
-- "my-project.my-dataset.input_data_abc123"

-- output_table might contain:
-- "my-project.my-dataset.output_result_xyz789"
```

**Why FQNs:**
- Tables are created in persistent datasets/databases
- Multiple users and workflows can run simultaneously
- Each workflow run creates uniquely named tables
- Tables persist for inspection and debugging

### API Execution Mode (Programmatic)

**When:** Workflow is executed via API or as an exported stored procedure

**Table name format:** Session tables (simple names)
- All providers: `tablename` (no project/dataset/database prefix)

**Example:**
```sql
-- input_table might contain:
-- "temp_input_1"

-- output_table might contain:
-- "temp_output_1"
```

**Why session tables:**
- Tables only exist for the duration of the procedure execution
- Simpler namespace management
- Automatic cleanup when session ends
- Faster execution (no persistent storage overhead)

### Writing Compatible Components

**Good news:** In most cases, you don't need special handling! The table reference syntax is identical for both FQN and session tables.

**Example that works in both contexts:**
```sql
EXECUTE IMMEDIATE '''
CREATE TABLE IF NOT EXISTS ''' || output_table || '''
AS SELECT * FROM ''' || input_table || '''
WHERE condition = true;
''';
```

This works because:
- If `input_table = "project.dataset.table1"` ‚Üí `FROM project.dataset.table1` ‚úì
- If `input_table = "table1"` ‚Üí `FROM table1` ‚úì

### When You Need Context Detection

You only need to detect the execution context if your component logic needs to behave differently based on table persistence.

**Detection pattern:**
```sql
-- Check if table name contains a dot (indicates FQN)
DECLARE is_fqn BOOL;
SET is_fqn = REGEXP_CONTAINS(input_table, r'\.');

IF is_fqn THEN
  -- UI execution: tables persist, can do post-processing
  -- Example: Create indexes, analyze statistics, etc.
ELSE
  -- API execution: session tables, keep processing simple
  -- Example: Skip optimization steps
END IF;
```

### Important Considerations

**Do:**
- ‚úÖ Use table name variables exactly as provided (`input_table`, `output_table`)
- ‚úÖ Assume either format could be passed
- ‚úÖ Use `EXECUTE IMMEDIATE` for dynamic table names
- ‚úÖ Test your component in both contexts

**Don't:**
- ‚ùå Hardcode table names
- ‚ùå Assume tables will always be FQNs
- ‚ùå Assume tables will always be session tables
- ‚ùå Parse or modify table names unless absolutely necessary

### Testing Both Contexts

**UI context (during development):**
```bash
# Deploy creates actual tables with FQNs
python carto_extension.py deploy --destination=myproject.mydataset
```

**API context (via tests):**
```bash
# Tests typically use session tables
python carto_extension.py test
```

Both should work without code changes!
