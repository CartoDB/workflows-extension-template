---
title: Writing Component Stored Procedures
description: Guide to implementing component logic in fullrun.sql and dryrun.sql with SQL patterns and best practices
version: 1.0.0
last-updated: 2025-01-27
depends-on: [component_metadata.md]
tags: [sql, stored-procedures, logic, implementation, patterns]
see-also: [glossary.md]
---

# Writing the component procedure

The component's logic should be implemented in the `fullrun.sql` and `dryrun.sql` files.

> **ðŸ’¡ Example:** See the [annotated template files](../components/template/src/) for detailed pattern explanations with comments.
>
> **ðŸ¤– For AI Agents:** Check [Quick Reference](./reference/quick-reference.md) for SQL pattern snippets and [Validation Rules](./reference/validation-rules.md) for SQL constraints.

## Dry runs

Workflows needs to perform a dry-run query before the actual execution of the workflow, in order to determine the resulting schema of each node.

For this, we need to create a `dryrun.sql` file that generates an empty table with the same schema as the actual component's result (defined in `fullrun.sql`) and returning 0 rows.

> ðŸ’¡ **Tip**
>
> The dry run code doesn't really need to be exactly the same as the full run. Functions that take longer to run can be avoided as long as the resulting schema is the same. For example, using `"uuid_string" AS uuid"` generates the same schema as `"GENERATE_UUID() AS uuid"` would generate.

Below you can see an example of a stored procedure built following the approach defined above. This procedure takes a table and generates a new one that includes and additional column with a unique identifier.

### `fullrun.sql`

```sql
        EXECUTE IMMEDIATE '''
        CREATE TABLE IF NOT EXISTS ''' || output || '''
        OPTIONS (expiration_timestamp = TIMESTAMP_ADD(CURRENT_TIMESTAMP(), INTERVAL 30 DAY))
        AS SELECT *, GENERATE_UUID() AS uuid
        FROM ''' || input || ';';
```

### `dryrun.sql`

```sql
        EXECUTE IMMEDIATE '''
        CREATE TABLE IF NOT EXISTS ''' || output || '''
        OPTIONS (
                expiration_timestamp = TIMESTAMP_ADD(CURRENT_TIMESTAMP(), INTERVAL 30 DAY))
        AS SELECT *, "uuid_string" AS uuid
        FROM ''' || input || '''
        WHERE 1 = 0;
        ''';
```

## Variables

For implementing the logic of the component, you will have available a set of variable matching the names of the parameters declared in the `metadata.json` file, with both input and output parameters. These variables will contain the values selected by the user when configuring the component as part of a workflow.

If you have declared environment variables in your component metadata (see [here](./component_metadata.md#cartoenvvars)), they will also be created and they will be available to be used from your code.

The `input` and `output` variables will contain the names of the input table that was configured for the component, and the name of the output one generated by the component. Parameters with those same names should have been declared in the corresponding `metadata.json`file.

Do not generate tables with names others than the ones provided in the variables corresponding to output parameters. Otherwise, those tables will not be used when the component output is connected to another component in the workflow.

## Referencing CARTO Analytics Toolbox

If your component uses CARTO Analytics Toolbox functions, you need to choose the appropriate approach based on how your component is structured:

### Approach 1: Using the @@analytics_toolbox_location@@ placeholder

Use the `@@analytics_toolbox_location@@` placeholder directly in your stored procedure SQL:

```sql
CREATE OR REPLACE PROCEDURE component_procedure()
BEGIN
  SELECT @@analytics_toolbox_location@@.FUNCTION_NAME(geography_column)
  FROM input_table;
END;
```

**How it works:**
- The placeholder `@@analytics_toolbox_location@@` is **replaced at extension installation time** in the Workflows UI
- The FQN is **baked into the stored procedure definition** when the extension is installed
- This substitution happens **once** when the user installs your extension
- The placeholder is replaced with the connection-specific location (e.g., `carto-un.carto`)

**When to use:**
- When the Analytics Toolbox FQN needs to be part of the procedure definition itself
- **Required for BigQuery** when function references are in the procedure body
- **BigQuery only** - not available for other providers

### Approach 2: Using cartoEnvVars

Declare `analyticsToolboxDataset` in your component's `cartoEnvVars` array:

```json
{
  "cartoEnvVars": ["analyticsToolboxDataset"],
  ...
}
```

Then use it as a variable in dynamic SQL:

```sql
EXECUTE IMMEDIATE '''
SELECT ''' || analyticsToolboxDataset || '''.FUNCTION_NAME(geography_column)
FROM ''' || input_table;
```

**How it works:**
- The variable `analyticsToolboxDataset` is **evaluated at workflow execution time**
- Workflows injects the correct value when building the SQL dynamically
- The value can adapt if connection settings change

**When to use:**
- When building SQL dynamically with `EXECUTE IMMEDIATE`
- Works across BigQuery, Snowflake, and Oracle
- When you need flexibility for the location to change after installation

### Key Differences

| Aspect | Placeholder | cartoEnvVars |
|--------|-------------|--------------|
| **When evaluated** | At extension installation (static) | At workflow execution (dynamic) |
| **Use case** | FQN in procedure definition | FQN in dynamic SQL |
| **Flexibility** | Fixed at installation | Can change with connection settings |
| **Availability** | **BigQuery only** | BigQuery, Snowflake, Oracle |
| **Context** | Baked into stored procedure | Evaluated when building SQL |

### For Local Testing

When running `python carto_extension.py test` or `deploy` locally, you can define the placeholder in a `.env` file:

```
analytics_toolbox_location=carto-un.carto
```

This is purely for your development workflow and has no effect on how the extension works for end users who install it from CARTO UI.


## Table names and API execution

When a workflow is run from the Workflows UI, table names of the tables created by its components are fully qualified. That means that, if your custom component is connected to an upstream components and uses a table from it, the name that it will received in the corresponding parameter will be a FQN (that is, in the form `project.dataset.table`). Output names received in the output parameters will also be FQNs.

However, when the workflow is run as a stored procedure (when exported or when executed via API), all tables created in components are session tables that are single names (that is, something like `tablename` instead of `project.dataset.table`). That means that inputs that come from other components, and also output table names, will be single-name tables.

You should prepare your component to deal with this situation. Check the input/output table names to see whether they are fully-qualified or not, and implement the corresponding logic to run in each case.
